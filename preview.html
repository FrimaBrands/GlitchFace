<!doctype html>
<html lang="en" class="dark">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>U N K N O W N // Preview</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/selfie_segmentation.js" crossorigin="anonymous"></script>
  <style>
    body { 
        font-family: 'JetBrains Mono', monospace; 
        background-color: #000000;
        color: #EAEAEA;
        margin: 0;
        overflow: hidden;
    }
    .main-stage {
        width: 100vw;
        height: 100vh;
    }
    #glcanvas {
        width: 100%;
        height: 100%;
        object-fit: cover;
    }
    .placeholder {
        display: flex;
        align-items: center;
        justify-content: center;
        width: 100%;
        height: 100%;
        font-size: 1.2rem;
        color: #555;
    }
  </style>
</head>
<body>

  <div class="main-stage flex-grow bg-black relative flex items-center justify-center overflow-hidden">
    <video id="video" playsinline muted class="hidden"></video>
    <video id="backgroundMedia" loop muted playsinline class="hidden absolute w-full h-full object-cover"></video>
    <img id="backgroundImage" class="hidden absolute w-full h-full object-cover" />
    <canvas id="glcanvas" class="w-full h-full object-cover"></canvas>
    <div id="placeholder" class="placeholder absolute inset-0">// WAITING FOR CONTROLS</div>
  </div>

  <script>
  (() => {
    // --- All WebGL, rendering, and body segmentation logic remains here. ---
    // --- It now listens for state changes from the Controls window via a BroadcastChannel. ---
    
    const vertSrc = `
      attribute vec2 a_pos;
      varying vec2 v_uv;
      void main() {
        v_uv = a_pos;
        gl_Position = vec4(a_pos * 2.0 - 1.0, 0.0, 1.0);
      }`;

    const fragSrc = `
      precision highp float;
      varying vec2 v_uv;

      // Textures
      uniform sampler2D u_tex;
      uniform sampler2D u_feedbackTex;
      uniform sampler2D u_maskTex;
      uniform sampler2D u_backgroundTex;

      // Global uniforms
      uniform vec2  u_resolution;
      uniform float u_time;
      uniform bool  u_isFeedbackPass;
      uniform bool  u_useBackground;
      uniform float u_maskFeather;

      // Opacity uniforms (not randomized)
      uniform float u_overallOpacity;
      uniform float u_flickerPerSecond;
      uniform float u_flickerDuration;
      uniform float u_flickerIntensity;
      uniform float u_flickerFade;

      // Seep uniforms (not randomized)
      uniform float u_seepAmount;
      uniform float u_seepIntensity;

      // Effect uniforms (randomized)
      uniform float u_feedbackAmount;
      uniform float u_feedbackZoom;
      uniform float u_glitchIntensity;
      uniform float u_glitchBlockSize;
      uniform float u_rgbShift;
      uniform float u_noiseAmount;
      uniform float u_noiseSpeed;
      uniform float u_vignette;
      
      // Smear uniforms
      uniform float u_globalSmearIntensity;
      uniform float u_globalSmearAngle;
      uniform float u_spotSmearIntensity;
      uniform float u_spotSmearDensity;
      uniform float u_spotSmearSize;


      // --- Helper Functions ---
      float rand(vec2 co){ return fract(sin(dot(co.xy, vec2(12.9898, 78.233))) * 43758.5453); }

      // Generic smear effect
      vec4 smear(sampler2D tex, vec2 uv, float intensity, float angle) {
        vec2 dir = vec2(cos(angle), sin(angle)) / u_resolution;
        vec4 color = vec4(0.0);
        const int samples = 16;
        for (int i = 0; i < samples; i++) {
          float t = float(i) / float(samples - 1);
          color += texture2D(tex, uv - dir * t * intensity);
        }
        return color / float(samples);
      }
      
      // Simple box blur for creating the 'seep' effect
      vec4 blur(sampler2D tex, vec2 uv, float radius) {
          vec4 acc = vec4(0.0);
          vec2 res = u_resolution.xy;
          float count = 0.0;
          // A 5x5 kernel for a small, efficient blur
          for(float x = -2.0; x <= 2.0; x++) {
              for(float y = -2.0; y <= 2.0; y++) {
                  vec2 offset = vec2(x, y) * radius / res;
                  acc += texture2D(tex, uv + offset);
                  count += 1.0;
              }
          }
          return acc / count;
      }

      void main() {
        vec2 uv = v_uv;
        vec4 videoColor = texture2D(u_tex, uv);

        // --- EFFECT PIPELINE ---
        vec4 feedbackColor = texture2D(u_feedbackTex, (uv - 0.5) * u_feedbackZoom + 0.5);
        vec4 effectedColor = mix(videoColor, feedbackColor, u_feedbackAmount);

        if (rand(vec2(floor(u_time * 15.0), 0.0)) < u_glitchIntensity) {
          vec2 block_uv = floor(uv * u_resolution.y / u_glitchBlockSize) / (u_resolution.y / u_glitchBlockSize);
          effectedColor = texture2D(u_tex, uv + vec2((rand(block_uv) - 0.5) * 0.1, 0.0));
        }

        float r = texture2D(u_tex, uv + vec2(u_rgbShift, 0.0)).r;
        float b = texture2D(u_tex, uv - vec2(u_rgbShift, 0.0)).b;
        effectedColor = vec4(r, effectedColor.g, b, effectedColor.a);

        effectedColor += (rand(uv + u_time * u_noiseSpeed) - 0.5) * u_noiseAmount;
        effectedColor.rgb *= 1.0 - u_vignette * distance(uv, vec2(0.5));

        vec4 smearedColor = effectedColor;
        
        vec2 grid_uv = floor(uv * u_spotSmearSize) / u_spotSmearSize;
        if (rand(grid_uv + floor(u_time * 5.0)) < u_spotSmearDensity) {
          smearedColor = smear(u_feedbackTex, uv, u_spotSmearIntensity, rand(grid_uv + 10.0) * 6.283);
        }

        float maskValue = texture2D(u_maskTex, uv).r;
        if (maskValue > 0.1) {
            vec4 globalSmear = smear(u_feedbackTex, uv, u_globalSmearIntensity, radians(u_globalSmearAngle));
            smearedColor = mix(smearedColor, globalSmear, 0.5);
        }


        float effectedGray = dot(smearedColor.rgb, vec3(0.299, 0.587, 0.114));
        vec4 effectedGrayscale = vec4(vec3(effectedGray), 1.0);

        // --- RENDER PASS LOGIC ---
        if (u_isFeedbackPass) {
          gl_FragColor = effectedGrayscale;
          return;
        }

        // --- FINAL COMPOSITION (Screen Pass) ---
        float originalGray = dot(videoColor.rgb, vec3(0.299, 0.587, 0.114));
        vec4 originalGrayscale = vec4(vec3(originalGray), 1.0);

        float baseMask = texture2D(u_maskTex, uv).r;
        float blurredMask = blur(u_maskTex, uv, u_seepIntensity).r;
        float seep = (1.0 - baseMask) * blurredMask * u_seepAmount;

        float finalMask = smoothstep(0.5 - u_maskFeather, 0.5 + u_maskFeather, baseMask);
        float silhouetteMask = clamp(finalMask + seep, 0.0, 1.0);

        // Calculate flicker
        float flickerMultiplier = 1.0;
        if (u_flickerPerSecond > 0.0) {
            float flickerCycle = 1.0 / u_flickerPerSecond;
            float flickerPhase = mod(u_time, flickerCycle);
            float flickerEdge = flickerCycle * u_flickerDuration;
            
            float fadeWidth = flickerCycle * u_flickerFade * 0.5;
            float isFlickerOff = 1.0 - smoothstep(flickerEdge - fadeWidth, flickerEdge + fadeWidth, flickerPhase);
            flickerMultiplier = 1.0 - (isFlickerOff * u_flickerIntensity);
        }

        // 'effectAmount' now combines opacity and flicker. This controls HOW MUCH effect is applied.
        float effectAmount = u_overallOpacity * flickerMultiplier;

        // The foreground color is a mix of the original body and the fully effected body.
        vec4 foregroundColor = mix(originalGrayscale, effectedGrayscale, effectAmount);
        
        // Determine the background color
        vec4 backgroundColor = u_useBackground ? texture2D(u_backgroundTex, uv) : originalGrayscale;

        // Mix the background with the calculated foreground color using the silhouette mask.
        gl_FragColor = mix(backgroundColor, foregroundColor, silhouetteMask);
      }`;

    const Renderer = {
      gl: null, program: null, textures: {}, framebuffers: {}, uniforms: {},
      maskCanvas: document.createElement('canvas'),
      init(canvas, vsSrc, fsSrc) {
        this.gl = canvas.getContext('webgl', { premultipliedAlpha: false, antialias: true });
        if (!this.gl) throw new Error('WebGL not available');
        const compile = (type, src) => {
          const s = this.gl.createShader(type); this.gl.shaderSource(s, src); this.gl.compileShader(s);
          if (!this.gl.getShaderParameter(s, this.gl.COMPILE_STATUS)) throw new Error(`Shader compile error: ${this.gl.getShaderInfoLog(s)}`);
          return s;
        };
        this.program = this.gl.createProgram();
        this.gl.attachShader(this.program, compile(this.gl.VERTEX_SHADER, vsSrc));
        this.gl.attachShader(this.program, compile(this.gl.FRAGMENT_SHADER, fsSrc));
        this.gl.linkProgram(this.program);
        if (!this.gl.getProgramParameter(this.program, this.gl.LINK_STATUS)) throw new Error(`Program link error: ${this.gl.getProgramInfoLog(this.program)}`);
        const buffer = this.gl.createBuffer(); this.gl.bindBuffer(this.gl.ARRAY_BUFFER, buffer);
        this.gl.bufferData(this.gl.ARRAY_BUFFER, new Float32Array([0,0, 1,0, 0,1, 1,1]), this.gl.STATIC_DRAW);
        const posLoc = this.gl.getAttribLocation(this.program, 'a_pos');
        this.gl.enableVertexAttribArray(posLoc); this.gl.vertexAttribPointer(posLoc, 2, this.gl.FLOAT, false, 0, 0);
        const createTex = () => {
          const tex = this.gl.createTexture(); this.gl.bindTexture(this.gl.TEXTURE_2D, tex);
          this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_WRAP_S, this.gl.CLAMP_TO_EDGE);
          this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_WRAP_T, this.gl.CLAMP_TO_EDGE);
          this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_MIN_FILTER, this.gl.LINEAR);
          this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_MAG_FILTER, this.gl.LINEAR);
          return tex;
        };
        this.textures = { video: createTex(), feedbackA: createTex(), feedbackB: createTex(), mask: createTex(), background: createTex() };
        this.framebuffers.feedback = this.gl.createFramebuffer();
        this.gl.useProgram(this.program);
      },
      getUniforms(keys) { keys.forEach(key => this.uniforms[key] = this.gl.getUniformLocation(this.program, key)); },
      fit(video) {
        if (!video.videoWidth) return;
        const { videoWidth: w, videoHeight: h } = video;
        const canvas = this.gl.canvas; if (canvas.width === w && canvas.height === h) return;
        canvas.width = w; canvas.height = h;
        const resizableTextures = [this.textures.video, this.textures.feedbackA, this.textures.feedbackB, this.textures.mask];
        for (const tex of resizableTextures) {
          this.gl.bindTexture(this.gl.TEXTURE_2D, tex);
          this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, w, h, 0, this.gl.RGBA, this.gl.UNSIGNED_BYTE, null);
        }
        this.gl.viewport(0, 0, w, h);
      },
      drawMask(video, segmentationMask) {
        const { videoWidth: W, videoHeight: H } = video;
        const ctx = this.maskCanvas.getContext('2d');
        if (this.maskCanvas.width !== W || this.maskCanvas.height !== H) { this.maskCanvas.width = W; this.maskCanvas.height = H; }
        ctx.fillStyle = 'black'; ctx.fillRect(0, 0, W, H);
        if (segmentationMask) { ctx.drawImage(segmentationMask, 0, 0, W, H); }
        this.gl.activeTexture(this.gl.TEXTURE2); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.mask);
        this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true);
        this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, this.gl.RGBA, this.gl.UNSIGNED_BYTE, this.maskCanvas);
      },
      render(appState) {
        const { video, t, state } = appState;
        this.gl.useProgram(this.program);
        this.gl.activeTexture(this.gl.TEXTURE0); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.video);
        this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true);
        this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, this.gl.RGBA, this.gl.UNSIGNED_BYTE, video);

        const bgEl = document.getElementById(state.misc.bgElementId);
        if (state.misc.useBg && bgEl && (bgEl.videoWidth || bgEl.naturalWidth)) {
            this.gl.activeTexture(this.gl.TEXTURE3); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.background);
            this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true);
            this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, this.gl.RGBA, this.gl.UNSIGNED_BYTE, bgEl);
        }
        this.gl.uniform2f(this.uniforms.u_resolution, this.gl.canvas.width, this.gl.canvas.height);
        this.gl.uniform1f(this.uniforms.u_time, t);

        for (const [key, value] of Object.entries(state.uniforms)) {
            this.gl.uniform1f(this.uniforms[key], value);
        }

        this.gl.uniform1i(this.uniforms.u_useBackground, state.misc.useBg);
        this.gl.uniform1f(this.uniforms.u_maskFeather, state.misc.maskFeather);
        this.gl.uniform1i(this.uniforms.u_tex, 0); this.gl.uniform1i(this.uniforms.u_feedbackTex, 1);
        this.gl.uniform1i(this.uniforms.u_maskTex, 2); this.gl.uniform1i(this.uniforms.u_backgroundTex, 3);
        
        this.gl.activeTexture(this.gl.TEXTURE1); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.feedbackA);
        this.gl.uniform1i(this.uniforms.u_isFeedbackPass, 1);
        this.gl.bindFramebuffer(this.gl.FRAMEBUFFER, this.framebuffers.feedback);
        this.gl.framebufferTexture2D(this.gl.FRAMEBUFFER, this.gl.COLOR_ATTACHMENT0, this.gl.TEXTURE_2D, this.textures.feedbackB, 0);
        this.gl.drawArrays(this.gl.TRIANGLE_STRIP, 0, 4);
        this.gl.uniform1i(this.uniforms.u_isFeedbackPass, 0);
        this.gl.bindFramebuffer(this.gl.FRAMEBUFFER, null);
        this.gl.drawArrays(this.gl.TRIANGLE_STRIP, 0, 4);
        [this.textures.feedbackA, this.textures.feedbackB] = [this.textures.feedbackB, this.textures.feedbackA];
      }
    };

    const BodySegmenter = {
      segmenter: null, lastResult: null, isReady: false, isBusy: false, lastUpdateTime: 0, minInterval: 60,
      async init() {
        if (this.isReady) return true;
        try {
          this.segmenter = new window.SelfieSegmentation({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${file}` });
          this.segmenter.setOptions({ modelSelection: 1 });
          this.segmenter.onResults(res => { this.lastResult = res.segmentationMask; this.isBusy = false; });
          await this.segmenter.initialize();
          this.isReady = true; return true;
        } catch (err) { console.error("Selfie Segmentation initialization failed:", err); return false; }
      },
      update(video) {
        const now = performance.now();
        if (this.isReady && !this.isBusy && (now - this.lastUpdateTime) > this.minInterval) {
          this.isBusy = true; this.lastUpdateTime = now;
          this.segmenter.send({ image: video });
        }
      }
    };
    
    const PreviewApp = {
      stream: null, rafId: null, video: null, glcanvas: null, currentState: null, channel: null,
      
      init() {
        this.video = document.getElementById('video');
        this.glcanvas = document.getElementById('glcanvas');
        this.channel = new BroadcastChannel('unknown_signal_processor');
        this.channel.onmessage = this.handleMessage.bind(this);
        window.addEventListener('beforeunload', () => this.channel.close());
      },
      
      handleMessage(event) {
        const { type, payload } = event.data;
        if (type === 'state_update') {
          this.currentState = payload;
          this.updateDOMFromState(this.currentState);
        } else if (type === 'command') {
          if (payload.action === 'start' && !this.rafId) this.start();
          if (payload.action === 'stop' && this.rafId) this.stop();
          if (payload.action === 'record') this.triggerRecording();
          if (payload.action === 'load_bg') this.loadBackground(payload.url, payload.fileType);
        }
      },
      
      updateDOMFromState(state) {
        if (!state || !state.misc) return;
        this.glcanvas.style.transform = state.misc.mirror ? 'scaleX(-1)' : 'none';
        const bgImg = document.getElementById('backgroundImage');
        const bgVid = document.getElementById('backgroundMedia');
        if (state.misc.useBg) {
          if(state.misc.bgElementId === 'backgroundImage') {
            bgImg.classList.remove('hidden');
            bgVid.classList.add('hidden');
            bgVid.pause();
          } else {
            bgVid.classList.remove('hidden');
            bgImg.classList.add('hidden');
            bgVid.play();
          }
        } else {
            bgImg.classList.add('hidden');
            bgVid.classList.add('hidden');
            bgVid.pause();
        }
      },
      
      loadBackground(url, fileType) {
        const bgImg = document.getElementById('backgroundImage');
        const bgVid = document.getElementById('backgroundMedia');
        if (fileType.startsWith('image/')) {
            bgImg.src = url;
            bgVid.src = '';
        } else if (fileType.startsWith('video/')) {
            bgVid.src = url;
            bgImg.src = '';
        }
      },

      async start() {
        if (!navigator.mediaDevices?.getUserMedia) {
          return this.sendStatus('getUserMedia not supported.', true);
        }
        try {
          this.sendStatus('Requesting camera...');
          this.stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user', width: 1280, height: 720 }, audio: false });
          this.video.srcObject = this.stream;
          await new Promise(resolve => { this.video.onloadedmetadata = resolve; });
          await this.video.play();
          
          document.getElementById('placeholder').style.display = 'none';

          this.sendStatus('Initializing GL...');
          const uniformKeys = [ 'u_resolution', 'u_time', 'u_isFeedbackPass', 'u_tex', 'u_feedbackTex', 'u_maskTex', 'u_backgroundTex', 'u_useBackground', 'u_maskFeather', 'u_overallOpacity', 'u_flickerPerSecond', 'u_flickerDuration', 'u_flickerIntensity', 'u_flickerFade', 'u_seepAmount', 'u_seepIntensity', 'u_feedbackAmount', 'u_feedbackZoom', 'u_glitchIntensity', 'u_glitchBlockSize', 'u_rgbShift', 'u_noiseAmount', 'u_noiseSpeed', 'u_vignette', 'u_globalSmearIntensity', 'u_globalSmearAngle', 'u_spotSmearIntensity', 'u_spotSmearDensity', 'u_spotSmearSize' ];
          Renderer.init(this.glcanvas, vertSrc, fragSrc);
          Renderer.getUniforms(uniformKeys);
          Renderer.fit(this.video);

          this.sendStatus('Initializing Body Segmentation...');
          if (!(await BodySegmenter.init())) return this.sendStatus('Segmentation Failed', true);

          this.sendStatus('Running');
          this.loop();
        } catch (err) {
          this.sendStatus(`Camera error: ${err.message}`, true);
        }
      },

      stop() {
        if (this.rafId) cancelAnimationFrame(this.rafId); this.rafId = null;
        if (this.stream) this.stream.getTracks().forEach(t => t.stop()); this.stream = null;
        document.getElementById('placeholder').style.display = 'flex';
        this.sendStatus('Terminated');
      },

      loop(time) {
        this.rafId = requestAnimationFrame(t => this.loop(t));
        if (!this.video.videoWidth || !Renderer.gl || !this.currentState) return;
        
        Renderer.fit(this.video);
        BodySegmenter.update(this.video);
        Renderer.drawMask(this.video, BodySegmenter.lastResult);
        
        const t = this.currentState.misc.animate ? time / 1000 : 0;
        
        Renderer.render({ video: this.video, t: t, state: this.currentState });
        
        const statusText = BodySegmenter.lastResult
          ? `BODY DETECTED [${Renderer.gl.canvas.width}x${Renderer.gl.canvas.height}]` 
          : `NO SIGNAL...`;
        this.sendStatus(statusText);
      },
      
      sendStatus(text, isError = false) {
        this.channel.postMessage({ type: 'status_update', payload: { text, isError } });
      },
      
      async triggerRecording() {
         try {
            const { blob, mime } = await this.recordCanvasFiveSeconds();
            const buffer = await blob.arrayBuffer();
            this.channel.postMessage({ type: 'recording_data', payload: { buffer, mime }}, [buffer]);
         } catch(e) {
            this.channel.postMessage({ type: 'recording_error', payload: { message: String(e) } });
         }
      },
      
      async recordCanvasFiveSeconds(preferMp4=false) {
        const canvas = this.glcanvas;
        if (!canvas || !canvas.captureStream) throw new Error('Canvas captureStream not supported');
        const stream = canvas.captureStream(30); const chunks = [];
        const optionsList = [
          { mimeType: 'video/webm;codecs=vp9', videoBitsPerSecond: 2_000_000 },
          { mimeType: 'video/webm;codecs=vp8', videoBitsPerSecond: 2_000_000 },
          { mimeType: 'video/webm', videoBitsPerSecond: 2_000_000 },
        ];
        if (preferMp4) optionsList.unshift({ mimeType: 'video/mp4;codecs=h264', videoBitsPerSecond: 2_000_000 });
        let rec = null; var mime = '';
        for (const opt of optionsList) {
          try {
            if (MediaRecorder.isTypeSupported(opt.mimeType)) {
                rec = new MediaRecorder(stream, opt); mime = rec.mimeType; break;
            }
          } catch (e) { /* try next */ }
        }
        if (!rec) throw new Error('MediaRecorder unavailable');
        await new Promise(res => setTimeout(res, 50));
        return await new Promise((resolve, reject) => {
          let stopped = false;
          rec.ondataavailable = e => { if (e.data && e.data.size) chunks.push(e.data); };
          rec.onerror = e => { if (!stopped) reject(e.error || e); };
          rec.onstop = () => {
            stopped = true;
            try { stream.getTracks().forEach(t => t.stop()); } catch {}
            const blob = new Blob(chunks, { type: mime }); resolve({ blob, mime });
          };
          rec.start(); setTimeout(() => { try { rec.stop(); } catch {} }, 5000);
        });
      }
    };
    
    PreviewApp.init();
  })();
  </script>

</body>
</html>
