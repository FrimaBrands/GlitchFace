<!doctype html>
<html lang="en" class="dark">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>U N K N O W N // Preview</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/selfie_segmentation.js" crossorigin="anonymous"></script>
  <style>
    body { 
        background-color: #000000;
        margin: 0;
        overflow: hidden;
    }
    .main-stage {
        width: 100vw;
        height: 100vh;
    }
    #glcanvas {
        width: 100%;
        height: 100%;
        object-fit: contain; /* or cover, depending on desired look */
    }
  </style>
</head>
<body>

  <div class="main-stage">
      <video id="video" playsinline muted class="hidden"></video>
      <video id="backgroundMedia" loop muted playsinline class="hidden absolute w-full h-full object-cover"></video>
      <img id="backgroundImage" class="hidden absolute w-full h-full object-cover" />
      <canvas id="glcanvas"></canvas>
  </div>

  <script>
  (() => {
    // --- APP ARCHITECTURE ---
    // This is the PREVIEW window. Its only job is to render the WebGL canvas.
    // It listens for state updates from the CONTROL window via a BroadcastChannel
    // and applies them to the renderer. It also manages the camera and the
    // PeerJS connection for remote recording.

    const channel = new BroadcastChannel('unknown_signal_channel');
    const els = {
        video: document.getElementById('video'),
        glcanvas: document.getElementById('glcanvas'),
        backgroundMedia: document.getElementById('backgroundMedia'),
        backgroundImage: document.getElementById('backgroundImage'),
    };

    let appState = {}; // This will hold the state received from the controls window

    const vertSrc = `attribute vec2 a_pos; varying vec2 v_uv; void main() { v_uv = a_pos; gl_Position = vec4(a_pos * 2.0 - 1.0, 0.0, 1.0); }`;
    const fragSrc = `precision highp float; varying vec2 v_uv; uniform sampler2D u_tex; uniform sampler2D u_feedbackTex; uniform sampler2D u_maskTex; uniform sampler2D u_backgroundTex; uniform vec2 u_resolution; uniform float u_time; uniform bool u_isFeedbackPass; uniform bool u_useBackground; uniform float u_maskFeather; uniform float u_overallOpacity; uniform float u_flickerPerSecond; uniform float u_flickerDuration; uniform float u_flickerIntensity; uniform float u_flickerFade; uniform float u_seepAmount; uniform float u_seepIntensity; uniform float u_feedbackAmount; uniform float u_feedbackZoom; uniform float u_glitchIntensity; uniform float u_glitchBlockSize; uniform float u_rgbShift; uniform float u_noiseAmount; uniform float u_noiseSpeed; uniform float u_vignette; uniform float u_globalSmearIntensity; uniform float u_globalSmearAngle; uniform float u_spotSmearIntensity; uniform float u_spotSmearDensity; uniform float u_spotSmearSize; float rand(vec2 co){ return fract(sin(dot(co.xy, vec2(12.9898, 78.233))) * 43758.5453); } vec4 smear(sampler2D tex, vec2 uv, float intensity, float angle) { vec2 dir = vec2(cos(angle), sin(angle)) / u_resolution; vec4 color = vec4(0.0); const int samples = 16; for (int i = 0; i < samples; i++) { float t = float(i) / float(samples - 1); color += texture2D(tex, uv - dir * t * intensity); } return color / float(samples); } vec4 blur(sampler2D tex, vec2 uv, float radius) { vec4 acc = vec4(0.0); vec2 res = u_resolution.xy; float count = 0.0; for(float x = -2.0; x <= 2.0; x++) { for(float y = -2.0; y <= 2.0; y++) { vec2 offset = vec2(x, y) * radius / res; acc += texture2D(tex, uv + offset); count += 1.0; } } return acc / count; } void main() { vec2 uv = v_uv; vec4 videoColor = texture2D(u_tex, uv); vec4 feedbackColor = texture2D(u_feedbackTex, (uv - 0.5) * u_feedbackZoom + 0.5); vec4 effectedColor = mix(videoColor, feedbackColor, u_feedbackAmount); if (rand(vec2(floor(u_time * 15.0), 0.0)) < u_glitchIntensity) { vec2 block_uv = floor(uv * u_resolution.y / u_glitchBlockSize) / (u_resolution.y / u_glitchBlockSize); effectedColor = texture2D(u_tex, uv + vec2((rand(block_uv) - 0.5) * 0.1, 0.0)); } float r = texture2D(u_tex, uv + vec2(u_rgbShift, 0.0)).r; float b = texture2D(u_tex, uv - vec2(u_rgbShift, 0.0)).b; effectedColor = vec4(r, effectedColor.g, b, effectedColor.a); effectedColor += (rand(uv + u_time * u_noiseSpeed) - 0.5) * u_noiseAmount; effectedColor.rgb *= 1.0 - u_vignette * distance(uv, vec2(0.5)); vec4 smearedColor = effectedColor; vec2 grid_uv = floor(uv * u_spotSmearSize) / u_spotSmearSize; if (rand(grid_uv + floor(u_time * 5.0)) < u_spotSmearDensity) { smearedColor = smear(u_feedbackTex, uv, u_spotSmearIntensity, rand(grid_uv + 10.0) * 6.283); } float maskValue = texture2D(u_maskTex, uv).r; if (maskValue > 0.1) { vec4 globalSmear = smear(u_feedbackTex, uv, u_globalSmearIntensity, radians(u_globalSmearAngle)); smearedColor = mix(smearedColor, globalSmear, 0.5); } float effectedGray = dot(smearedColor.rgb, vec3(0.299, 0.587, 0.114)); vec4 effectedGrayscale = vec4(vec3(effectedGray), 1.0); if (u_isFeedbackPass) { gl_FragColor = effectedGrayscale; return; } float originalGray = dot(videoColor.rgb, vec3(0.299, 0.587, 0.114)); vec4 originalGrayscale = vec4(vec3(originalGray), 1.0); float baseMask = texture2D(u_maskTex, uv).r; float blurredMask = blur(u_maskTex, uv, u_seepIntensity).r; float seep = (1.0 - baseMask) * blurredMask * u_seepAmount; float finalMask = smoothstep(0.5 - u_maskFeather, 0.5 + u_maskFeather, baseMask); float silhouetteMask = clamp(finalMask + seep, 0.0, 1.0); float flickerMultiplier = 1.0; if (u_flickerPerSecond > 0.0) { float flickerCycle = 1.0 / u_flickerPerSecond; float flickerPhase = mod(u_time, flickerCycle); float flickerEdge = flickerCycle * u_flickerDuration; float fadeWidth = flickerCycle * u_flickerFade * 0.5; float isFlickerOff = 1.0 - smoothstep(flickerEdge - fadeWidth, flickerEdge + fadeWidth, flickerPhase); flickerMultiplier = 1.0 - (isFlickerOff * u_flickerIntensity); } float effectAmount = u_overallOpacity * flickerMultiplier; vec4 foregroundColor = mix(originalGrayscale, effectedGrayscale, effectAmount); vec4 backgroundColor = u_useBackground ? texture2D(u_backgroundTex, uv) : originalGrayscale; gl_FragColor = mix(backgroundColor, foregroundColor, silhouetteMask); }`;
    const uniformKeys = [ 'u_resolution', 'u_time', 'u_isFeedbackPass', 'u_tex', 'u_feedbackTex', 'u_maskTex', 'u_backgroundTex', 'u_useBackground', 'u_maskFeather', 'overallOpacity', 'flickerPerSecond', 'flickerDuration', 'flickerIntensity', 'flickerFade', 'seepAmount', 'seepIntensity', 'feedbackAmount', 'feedbackZoom', 'glitchIntensity', 'glitchBlockSize', 'rgbShift', 'noiseAmount', 'noiseSpeed', 'vignette', 'globalSmearIntensity', 'globalSmearAngle', 'spotSmearIntensity', 'spotSmearDensity', 'spotSmearSize', 'jitterAmount', 'jitterSpeed' ];
    const jitterState = {};

    const Renderer = {
      gl: null, program: null, textures: {}, framebuffers: {}, uniforms: {}, maskCanvas: document.createElement('canvas'),
      init(canvas, vsSrc, fsSrc) { /* ... full renderer init code from original index.html ... */ this.gl = canvas.getContext('webgl', { premultipliedAlpha: false, antialias: true }); if (!this.gl) throw new Error('WebGL not available'); const compile = (type, src) => { const s = this.gl.createShader(type); this.gl.shaderSource(s, src); this.gl.compileShader(s); if (!this.gl.getShaderParameter(s, this.gl.COMPILE_STATUS)) throw new Error(`Shader compile error: ${this.gl.getShaderInfoLog(s)}`); return s; }; this.program = this.gl.createProgram(); this.gl.attachShader(this.program, compile(this.gl.VERTEX_SHADER, vsSrc)); this.gl.attachShader(this.program, compile(this.gl.FRAGMENT_SHADER, fsSrc)); this.gl.linkProgram(this.program); if (!this.gl.getProgramParameter(this.program, this.gl.LINK_STATUS)) throw new Error(`Program link error: ${this.gl.getProgramInfoLog(this.program)}`); const buffer = this.gl.createBuffer(); this.gl.bindBuffer(this.gl.ARRAY_BUFFER, buffer); this.gl.bufferData(this.gl.ARRAY_BUFFER, new Float32Array([0,0, 1,0, 0,1, 1,1]), this.gl.STATIC_DRAW); const posLoc = this.gl.getAttribLocation(this.program, 'a_pos'); this.gl.enableVertexAttribArray(posLoc); this.gl.vertexAttribPointer(posLoc, 2, this.gl.FLOAT, false, 0, 0); const createTex = () => { const tex = this.gl.createTexture(); this.gl.bindTexture(this.gl.TEXTURE_2D, tex); this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_WRAP_S, this.gl.CLAMP_TO_EDGE); this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_WRAP_T, this.gl.CLAMP_TO_EDGE); this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_MIN_FILTER, this.gl.LINEAR); this.gl.texParameteri(this.gl.TEXTURE_2D, this.gl.TEXTURE_MAG_FILTER, this.gl.LINEAR); return tex; }; this.textures = { video: createTex(), feedbackA: createTex(), feedbackB: createTex(), mask: createTex(), background: createTex() }; this.framebuffers.feedback = this.gl.createFramebuffer(); this.gl.useProgram(this.program); uniformKeys.forEach(key => this.uniforms[key] = this.gl.getUniformLocation(this.program, key.startsWith('u_') ? key : `u_${key}`)); },
      fit(video) { if (!video.videoWidth) return; const { videoWidth: w, videoHeight: h } = video; const canvas = this.gl.canvas; if (canvas.width === w && canvas.height === h) return; canvas.width = w; canvas.height = h; const resizableTextures = [this.textures.video, this.textures.feedbackA, this.textures.feedbackB, this.textures.mask]; for (const tex of resizableTextures) { this.gl.bindTexture(this.gl.TEXTURE_2D, tex); this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, w, h, 0, this.gl.RGBA, this.gl.UNSIGNED_BYTE, null); } this.gl.viewport(0, 0, w, h); },
      drawMask(video, segmentationMask) { const { videoWidth: W, videoHeight: H } = video; const ctx = this.maskCanvas.getContext('2d'); if (this.maskCanvas.width !== W || this.maskCanvas.height !== H) { this.maskCanvas.width = W; this.maskCanvas.height = H; } ctx.fillStyle = 'black'; ctx.fillRect(0, 0, W, H); if (segmentationMask) { ctx.drawImage(segmentationMask, 0, 0, W, H); } this.gl.activeTexture(this.gl.TEXTURE2); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.mask); this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true); this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, this.gl.RGBA, this.gl.UNSIGNED_BYTE, this.maskCanvas); },
      render(video, t) { if (!appState.controls) return; this.gl.useProgram(this.program); this.gl.activeTexture(this.gl.TEXTURE0); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.video); this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true); this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, this.gl.RGBA, this.gl.UNSIGNED_BYTE, video); const backgroundEl = !els.backgroundImage.classList.contains('hidden') ? els.backgroundImage : els.backgroundMedia; if (appState.checkboxes?.useBgChk && (backgroundEl.videoWidth || backgroundEl.naturalWidth || backgroundEl.width)) { this.gl.activeTexture(this.gl.TEXTURE3); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.background); this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true); this.gl.texImage2D(this.gl.TEXTURE_2D, 0, this.gl.RGBA, this.gl.RGBA, this.gl.UNSIGNED_BYTE, backgroundEl); } this.gl.uniform2f(this.uniforms.u_resolution, this.gl.canvas.width, this.gl.canvas.height); this.gl.uniform1f(this.uniforms.u_time, t); const isJitterActive = appState.toggles?.Jitter; if (isJitterActive) { const amount = parseFloat(appState.controls.Jitter.jitterAmount); const speed = parseFloat(appState.controls.Jitter.jitterSpeed); for (const groupKey in appState.controls) { if (groupKey === 'Jitter' || !appState.toggles[groupKey]) continue; for (const controlKey in appState.controls[groupKey]) { if (!jitterState[groupKey]) jitterState[groupKey] = {}; if (!jitterState[groupKey][controlKey]) jitterState[groupKey][controlKey] = { base: 0, current: 0}; jitterState[groupKey][controlKey].base = parseFloat(appState.controls[groupKey][controlKey]); const range = 1.0; const offset = (Math.sin(t * speed + controlKey.length) * Math.cos(t * speed * 0.7 + groupKey.length)) * amount * range; jitterState[groupKey][controlKey].current = jitterState[groupKey][controlKey].base + offset; } } } for (const groupKey in appState.controls) { const isActive = appState.toggles?.[groupKey]; for (const controlKey in appState.controls[groupKey]) { let val = isActive ? parseFloat(appState.controls[groupKey][controlKey]) : 0; if (isActive && isJitterActive && jitterState[groupKey]?.[controlKey]) val = jitterState[groupKey][controlKey].current; if (this.uniforms[controlKey]) this.gl.uniform1f(this.uniforms[controlKey], val); } } for (const groupKey in appState.globalControls) { for (const controlKey in appState.globalControls[groupKey]) { if (this.uniforms[controlKey]) this.gl.uniform1f(this.uniforms[controlKey], parseFloat(appState.globalControls[groupKey][controlKey])); } } this.gl.uniform1i(this.uniforms.u_useBackground, appState.checkboxes?.useBgChk ?? false); this.gl.uniform1f(this.uniforms.u_maskFeather, appState.misc?.maskFeather ?? 0.15); this.gl.uniform1i(this.uniforms.u_tex, 0); this.gl.uniform1i(this.uniforms.u_feedbackTex, 1); this.gl.uniform1i(this.uniforms.u_maskTex, 2); this.gl.uniform1i(this.uniforms.u_backgroundTex, 3); this.gl.activeTexture(this.gl.TEXTURE1); this.gl.bindTexture(this.gl.TEXTURE_2D, this.textures.feedbackA); this.gl.uniform1i(this.uniforms.u_isFeedbackPass, 1); this.gl.bindFramebuffer(this.gl.FRAMEBUFFER, this.framebuffers.feedback); this.gl.framebufferTexture2D(this.gl.FRAMEBUFFER, this.gl.COLOR_ATTACHMENT0, this.gl.TEXTURE_2D, this.textures.feedbackB, 0); this.gl.drawArrays(this.gl.TRIANGLE_STRIP, 0, 4); this.gl.uniform1i(this.uniforms.u_isFeedbackPass, 0); this.gl.bindFramebuffer(this.gl.FRAMEBUFFER, null); this.gl.drawArrays(this.gl.TRIANGLE_STRIP, 0, 4); [this.textures.feedbackA, this.textures.feedbackB] = [this.textures.feedbackB, this.textures.feedbackA]; }
    };
    
    const BodySegmenter = { /* ... full BodySegmenter code from original ... */ segmenter: null, lastResult: null, isReady: false, isBusy: false, lastUpdateTime: 0, minInterval: 60, async init() { if (this.isReady) return true; try { this.segmenter = new window.SelfieSegmentation({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${file}` }); this.segmenter.setOptions({ modelSelection: 1 }); this.segmenter.onResults(res => { this.lastResult = res.segmentationMask; this.isBusy = false; }); await this.segmenter.initialize(); this.isReady = true; return true; } catch (err) { console.error("Selfie Segmentation initialization failed:", err); return false; } }, update(video) { const now = performance.now(); if (this.isReady && !this.isBusy && (now - this.lastUpdateTime) > this.minInterval) { this.isBusy = true; this.lastUpdateTime = now; this.segmenter.send({ image: video }); } } };

    const App = {
      stream: null, rafId: null, lastRandomizeTime: 0, lastPresetIterateTime: 0, currentPresetIndex: 0,
      async start() { if (!navigator.mediaDevices?.getUserMedia) return this.sendStatus('Error', true, 'getUserMedia not supported.'); this.sendStatus('Requesting camera...'); try { this.stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user', width: 1280, height: 720 }, audio: false }); els.video.srcObject = this.stream; await new Promise(resolve => { els.video.onloadedmetadata = resolve; }); await els.video.play(); this.sendStatus('Initializing GL...'); Renderer.init(els.glcanvas, vertSrc, fragSrc); Renderer.fit(els.video); this.sendStatus('Initializing Body Segmentation...'); if (!(await BodySegmenter.init())) return this.sendStatus('Segmentation Failed', true); this.sendStatus('Running', false, '', true); this.loop(); } catch (err) { console.error(err); this.sendStatus('Error', true, `Camera error: ${(err.name === 'NotAllowedError') ? 'Permission denied.' : err.message}`); } },
      stop() { if (this.rafId) cancelAnimationFrame(this.rafId); this.rafId = null; if (this.stream) this.stream.getTracks().forEach(t => t.stop()); this.stream = null; this.sendStatus('Terminated', false, '', false); },
      loop(time) { this.rafId = requestAnimationFrame(t => this.loop(t)); if (!els.video.videoWidth || !Renderer.gl) return; Renderer.fit(els.video); BodySegmenter.update(els.video); Renderer.drawMask(els.video, BodySegmenter.lastResult); const t = appState.checkboxes?.animateChk ? time / 1000 : 0; Renderer.render(els.video, t); const statusText = BodySegmenter.lastResult ? `BODY DETECTED [${Renderer.gl.canvas.width}x${Renderer.gl.canvas.height}]` : `NO SIGNAL...`; this.sendStatus(statusText, false, '', true); },
      init() { Renderer.init(els.glcanvas, vertSrc, fragSrc); },
      sendStatus(status, isError=false, message='', isRunning=false) { channel.postMessage({ type: 'app_status', payload: { status, isError, message, isRunning }}); }
    };
    
    // --- Message Handling ---
    channel.addEventListener('message', (ev) => {
        const { type, payload } = ev.data;
        switch (type) {
            case 'state_update':
                appState = payload;
                els.glcanvas.style.transform = appState.checkboxes?.mirrorChk ? 'scaleX(-1)' : 'none';
                break;
            case 'command':
                if (payload === 'start') App.start();
                else if (payload === 'stop') App.stop();
                break;
            case 'background_update':
                handleBackgroundUpdate(payload);
                break;
            case 'remote_command':
                if (payload === 'arm') RemoteTrigger.armRemote();
                else if (payload === 'disarm') RemoteTrigger.disarmRemote();
                else if (payload === 'reset') RemoteTrigger.resetRemote();
                break;
        }
    });

    function handleBackgroundUpdate(payload) {
        const url = payload.fileData; // It's a Data URL
        if (payload.fileType.startsWith('image/')) {
            els.backgroundImage.src = url;
            els.backgroundImage.classList.remove('hidden');
            els.backgroundMedia.classList.add('hidden');
            els.backgroundMedia.pause();
            els.backgroundMedia.src = '';
        } else if (payload.fileType.startsWith('video/')) {
            els.backgroundMedia.src = url;
            els.backgroundMedia.play();
            els.backgroundMedia.classList.remove('hidden');
            els.backgroundImage.classList.add('hidden');
            els.backgroundImage.src = '';
        }
    }

    App.init();
  })();
  </script>

<script>
// --- Remote Trigger Module ---
// This now lives in the PREVIEW window, as it needs direct access to the canvas for recording.
(() => {
    const PEERJS_URL = 'https://unpkg.com/peerjs@1.5.2/dist/peerjs.min.js';
    const channel = new BroadcastChannel('unknown_signal_channel');
    let peer = null, sessionToken = null, armed = false, isRecording = false;
    const connections = new Map();
    const els = { canvas: document.getElementById('glcanvas') };

    function broadcastStatus() {
        channel.postMessage({
            type: 'remote_status',
            payload: {
                armed,
                isRecording,
                peerId: peer ? peer.id : null,
                sessionToken,
                connections: connections.size,
                error: null // Can be implemented
            }
        });
    }

    function lazyLoadPeerJS() { /* ... full lazyLoadPeerJS code from original ... */ return new Promise((resolve, reject) => { if (window.Peer) return resolve(); const s = document.createElement('script'); s.src = PEERJS_URL; s.crossOrigin = 'anonymous'; s.onload = () => resolve(); s.onerror = e => reject(new Error('Failed to load PeerJS')); document.body.appendChild(s); }); }
    function randomToken(len=24) { /* ... full randomToken code from original ... */ const arr = new Uint8Array(len); crypto.getRandomValues(arr); return Array.from(arr, b => b.toString(16).padStart(2,'0')).join(''); }

    async function recordCanvasFiveSeconds(preferMp4=false) { /* ... full recordCanvasFiveSeconds code from original ... */ const canvas = els.canvas; if (!canvas || !canvas.captureStream) throw new Error('Canvas captureStream not supported'); const stream = canvas.captureStream(30); const chunks = []; const optionsList = [ { mimeType: 'video/webm;codecs=vp9', videoBitsPerSecond: 2_000_000 }, { mimeType: 'video/webm;codecs=vp8', videoBitsPerSecond: 2_000_000 }, { mimeType: 'video/webm', videoBitsPerSecond: 2_000_000 }, ]; if (preferMp4) optionsList.unshift({ mimeType: 'video/mp4;codecs=h264', videoBitsPerSecond: 2_000_000 }); let rec = null; var mime = ''; for (const opt of optionsList) { try { if (MediaRecorder.isTypeSupported(opt.mimeType)) { rec = new MediaRecorder(stream, opt); mime = rec.mimeType; break; } } catch (e) { /* try next */ } } if (!rec) throw new Error('MediaRecorder unavailable for desired formats'); await new Promise(res => setTimeout(res, 50)); return await new Promise((resolve, reject) => { let stopped = false; rec.ondataavailable = e => { if (e.data && e.data.size) chunks.push(e.data); }; rec.onerror = e => { if (!stopped) reject(e.error || e); }; rec.onstop = () => { stopped = true; try { stream.getTracks().forEach(t => t.stop()); } catch {} const blob = new Blob(chunks, { type: mime }); resolve({ blob, mime }); }; rec.start(); setTimeout(() => { try { rec.stop(); } catch {} }, 5000); }); }
    async function onMessage(msg, conn) { /* ... full onMessage code from original ... */ try { const data = typeof msg === 'string' ? JSON.parse(msg) : msg; if (data.type === 'hello') { if (data.token !== sessionToken) { conn.send(JSON.stringify({ type:'error', message: 'Invalid token.' })); setTimeout(() => { try { conn.close(); } catch {} }, 100); return; } conn.capabilities = { supportsWebM: !!data.supportsWebM }; conn.send(JSON.stringify({ type:'helloAck', ok:true })); return; } if (data.type === 'startRecording') { if (!armed) return conn.send(JSON.stringify({ type: 'error', message: 'Remote is not armed.' })); if (isRecording) return conn.send(JSON.stringify({ type: 'error', message: 'Host is busy recording.' })); isRecording = true; broadcastStatus(); const preferMp4 = conn.capabilities ? !conn.capabilities.supportsWebM : false; let recording; try { recording = await recordCanvasFiveSeconds(preferMp4); } catch (e) { conn.send(JSON.stringify({ type:'error', message: String(e) })); isRecording = false; broadcastStatus(); return; } const { blob, mime } = recording; const arrayBuf = await blob.arrayBuffer(); const total = arrayBuf.byteLength; const filename = `unknown_${Date.now()}.${mime.includes('mp4')?'mp4':'webm'}`; conn.send(JSON.stringify({ type:'recordingMeta', mime, totalBytes: total, filename })); const CHUNK = 64 * 1024; let sent = 0; const channel = conn.dataChannel; const threshold = 1 * 1024 * 1024; while (sent < total) { if (channel && channel.bufferedAmount > threshold) { await new Promise(r => { channel.onbufferedamountlow = r; }); } const end = Math.min(total, sent + CHUNK); const chunk = arrayBuf.slice(sent, end); conn.send(chunk); sent = end; if (sent % (CHUNK * 10) === 0) { conn.send(JSON.stringify({ type:'progress', sent })); } } conn.send(JSON.stringify({ type:'recordingEnd' })); isRecording = false; broadcastStatus(); return; } } catch (e) { console.error('Remote message error', e); isRecording = false; broadcastStatus(); } }
    
    const RemoteTrigger = {
        async armRemote() {
            await lazyLoadPeerJS();
            sessionToken = randomToken(16);
            peer = new window.Peer(undefined, { debug: 1, config: { 'iceServers': [ { urls: 'stun:stun.l.google.com:19302' }, { urls: 'stun:stun1.l.google.com:19302' }, ] } });

            peer.on('open', id => { armed = true; broadcastStatus(); });
            peer.on('connection', (conn) => { connections.set(conn.peer, conn); broadcastStatus(); conn.on('data', (msg) => onMessage(msg, conn)); conn.on('close', () => { connections.delete(conn.peer); broadcastStatus(); }); conn.on('error', (e) => { console.error('Peer connection error:', e); connections.delete(conn.peer); broadcastStatus(); }); });
            peer.on('error', (err) => { console.error('PeerJS error:', err); if (err.type === 'peer-unavailable' || err.type === 'network') { this.disarmRemote(); } broadcastStatus(); });
            
            armed = true;
            broadcastStatus();
        },
        disarmRemote() {
            armed = false; isRecording = false;
            for (const conn of connections.values()) { try { conn.close(); } catch {} }
            connections.clear();
            if (peer) try { peer.destroy(); } catch {}
            peer = null; sessionToken = null;
            broadcastStatus();
        },
        resetRemote() {
            this.disarmRemote();
            setTimeout(() => this.armRemote(), 100);
        }
    };
    window.RemoteTrigger = RemoteTrigger;
})();
</script>

</body>
</html>
